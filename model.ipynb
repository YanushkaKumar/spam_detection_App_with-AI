{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1331586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English dataset...\n",
      "English dataset loaded: 5572 samples\n",
      "Label distribution: {0: 4825, 1: 747}\n",
      "Training English model with enhanced features...\n",
      "English Model Performance:\n",
      "Ensemble Accuracy: 0.9865\n",
      "Combined Accuracy: 0.9892\n",
      "\n",
      "Classification Report (Combined):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       966\n",
      "           1       1.00      0.92      0.96       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.96      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n",
      "\n",
      "Loading Sinhala dataset...\n",
      "Sinhala dataset loaded: 653 samples\n",
      "Label distribution: {1: 525, 0: 128}\n",
      "Training Sinhala model with enhanced features...\n",
      "Enhanced Sinhala dataset: 793 samples\n",
      "Label distribution: {1: 543, 0: 250}\n",
      "Sinhala Model Performance:\n",
      "Ensemble Accuracy: 1.0000\n",
      "Combined Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      1.00      1.00       109\n",
      "\n",
      "    accuracy                           1.00       159\n",
      "   macro avg       1.00      1.00      1.00       159\n",
      "weighted avg       1.00      1.00      1.00       159\n",
      "\n",
      "All models saved successfully!\n",
      "\n",
      "==================================================\n",
      "Training completed! Models saved.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Improved Spam Detection Model with Enhanced Features - FIXED\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Try to import NLTK components with fallback\n",
    "try:\n",
    "    import nltk\n",
    "    # Try to download NLTK data\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        NLTK_AVAILABLE = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: NLTK download failed: {e}\")\n",
    "        print(\"Falling back to basic text processing...\")\n",
    "        NLTK_AVAILABLE = False\n",
    "except ImportError:\n",
    "    print(\"Warning: NLTK not available. Using basic text processing...\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "class ImprovedSpamDetector:\n",
    "    def __init__(self):\n",
    "        self.english_model = None\n",
    "        self.sinhala_model = None\n",
    "        self.english_vectorizer = None\n",
    "        self.sinhala_vectorizer = None\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features(self, text, language='en'):\n",
    "        \"\"\"Extract additional features from text\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Basic text statistics\n",
    "        features['length'] = len(text)\n",
    "        features['word_count'] = len(text.split())\n",
    "        features['char_count'] = len([c for c in text if c.isalpha()])\n",
    "        features['digit_count'] = len([c for c in text if c.isdigit()])\n",
    "        features['upper_case_count'] = len([c for c in text if c.isupper()])\n",
    "        features['punctuation_count'] = len([c for c in text if c in string.punctuation])\n",
    "        \n",
    "        # Ratios (avoid division by zero)\n",
    "        if len(text) > 0:\n",
    "            features['digit_ratio'] = features['digit_count'] / len(text)\n",
    "            features['upper_ratio'] = features['upper_case_count'] / len(text)\n",
    "            features['punct_ratio'] = features['punctuation_count'] / len(text)\n",
    "        else:\n",
    "            features['digit_ratio'] = 0\n",
    "            features['upper_ratio'] = 0\n",
    "            features['punct_ratio'] = 0\n",
    "            \n",
    "        # Special characters\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['dollar_count'] = text.count('$')\n",
    "        features['url_count'] = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "        features['email_count'] = len(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text))\n",
    "        features['phone_count'] = len(re.findall(r'\\b\\d{3}-\\d{3}-\\d{4}\\b|\\b\\d{10}\\b', text))\n",
    "        \n",
    "        # Spam keywords (expanded for better detection)\n",
    "        spam_keywords_en = ['free', 'win', 'winner', 'cash', 'prize', 'urgent', 'congratulations', \n",
    "                           'offer', 'deal', 'discount', 'limited', 'act now', 'call now', 'click here',\n",
    "                           'guarantee', 'amazing', 'incredible', 'fantastic', 'bonus', 'reward']\n",
    "        \n",
    "        spam_keywords_si = ['නොමිලේ', 'ජයග්‍රහණය', 'ත්‍යාගය', 'මුදල්', 'හදිසි', 'සුභ පැතුම්', \n",
    "                           'දිනන', 'අවස්ථාවක්', 'ලක්ෂ', 'වාසියෙන්', 'තෝරාගෙන', 'බැංකු', \n",
    "                           'සත්‍යාපනය', 'ලියාපදිංචි', 'බාගන්න', 'දැන්ම', 'පිවිසෙන්න']\n",
    "        \n",
    "        if language == 'en':\n",
    "            spam_words = spam_keywords_en\n",
    "        else:\n",
    "            spam_words = spam_keywords_si\n",
    "            \n",
    "        text_lower = text.lower()\n",
    "        features['spam_keywords_count'] = sum(1 for word in spam_words if word in text_lower)\n",
    "        \n",
    "        # Average word length\n",
    "        words = text.split()\n",
    "        if words:\n",
    "            features['avg_word_length'] = sum(len(word) for word in words) / len(words)\n",
    "        else:\n",
    "            features['avg_word_length'] = 0\n",
    "            \n",
    "        return list(features.values())\n",
    "    \n",
    "    def basic_tokenize(self, text):\n",
    "        \"\"\"Basic tokenization fallback when NLTK is not available\"\"\"\n",
    "        # Simple word tokenization using regex\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return words\n",
    "    \n",
    "    def get_english_stopwords(self):\n",
    "        \"\"\"Get English stopwords with fallback\"\"\"\n",
    "        if NLTK_AVAILABLE:\n",
    "            try:\n",
    "                return set(stopwords.words('english'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback stopwords list\n",
    "        return {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n",
    "            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "            'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', \n",
    "            'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "            'further', 'then', 'once'\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text, language='en'):\n",
    "        \"\"\"Enhanced text preprocessing with NLTK fallback\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # For English, remove stopwords but keep some important ones for spam detection\n",
    "        if language == 'en':\n",
    "            stop_words = self.get_english_stopwords()\n",
    "            # Keep these words as they might be important for spam detection\n",
    "            keep_words = {'free', 'win', 'money', 'cash', 'offer', 'deal', 'urgent', 'now'}\n",
    "            stop_words = stop_words - keep_words\n",
    "            \n",
    "            if NLTK_AVAILABLE:\n",
    "                try:\n",
    "                    words = word_tokenize(text)\n",
    "                except:\n",
    "                    words = self.basic_tokenize(text)\n",
    "            else:\n",
    "                words = self.basic_tokenize(text)\n",
    "                \n",
    "            text = ' '.join([word for word in words if word not in stop_words])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_realistic_sinhala_dataset(self, original_df):\n",
    "        \"\"\"Create a more realistic Sinhala dataset by parsing the raw text\"\"\"\n",
    "        # Parse the raw text you provided\n",
    "        raw_text = \"\"\"spam දැන්ම පිවිසෙන්න දිනන අවස්ථාවක් නැවත නොලැබේ ඔබට ලක්ෂ 100ක් නව ජංගම දුරකථනයක් දිනාගන්න ඔබ ඉතා වාසියෙන් තෝරාගෙන ඇත ඔබගේ බැංකු ගිණුම සත්‍යාපනය කරන්න spam දුරකථන අංකය භාවිතා කරන්න ඔබට පණිවුඩයක් ඇත ඔබට පණිවුඩයක් ඇත දැන්ම ලියාපදිංචි වන්න ඔබගේ බැංකු ගිණුම සත්‍යාපනය කරන්න ඔබ ඉතා වාසියෙන් තෝරාගෙන ඇත මෙම ඇප් එක බාගන්න ham මට නව පොතක් කියවන්න තියෙනවා අපි සෙරිනට යමුද ඔබට සුබ දවසක් වේවා කාලෙකින් හමුවුණා අද කඳවුරට යමු ham මම පාසලට යනවා ඊයේ චිත්‍රපටය ගොඩක් හොඳයි දැන්ම එන්න අපි එකට යමු අම්මා ආපහු ආවා\"\"\"\n",
    "        \n",
    "        # Split by spam/ham markers\n",
    "        segments = re.split(r'\\b(spam|ham)\\b', raw_text)\n",
    "        \n",
    "        Messages = []\n",
    "        current_Label = None\n",
    "        \n",
    "        for segment in segments:\n",
    "            segment = segment.strip()\n",
    "            if segment == 'spam':\n",
    "                current_Label = 1\n",
    "            elif segment == 'ham':\n",
    "                current_Label = 0\n",
    "            elif segment and current_Label is not None:\n",
    "                # Split long segments into individual Messages\n",
    "                sentences = re.split(r'[.!?]|\\s{2,}', segment)\n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if len(sentence) > 10:  # Only keep meaningful sentences\n",
    "                        Messages.append({'Message': sentence, 'Label': current_Label})\n",
    "        \n",
    "        # Create additional spam and ham examples\n",
    "        spam_templates = [\n",
    "            \"දැන්ම ලියාপදිංචි වන්න ඔබට ලක්ෂ {} දිනාගත හැකිය\",\n",
    "            \"නොමිලේ {} ලබාගන්න දැන්ම කාර්ඩ් අංකය ඇතුළත් කරන්න\",\n",
    "            \"ඔබ ජයග්‍රහණය කර ඇත! {} ත්‍යාගය ලබාගන්න\",\n",
    "            \"වාසියෙන් තෝරාගත් ඔබට විශේෂ {} අවස්ථාව\",\n",
    "            \"බැංකු ගිණුම සත්‍යාපනය කරන්න {} ලබාගන්න\",\n",
    "            \"හදිසි! {} දිනන අවස්ථාව අවසන් වීමට කාලය සීමිතයි\"\n",
    "        ]\n",
    "        \n",
    "        ham_templates = [\n",
    "            \"අද පාසලට යන්න ඕනේ {} ගෙන්න අමතක කරන්න එපා\",\n",
    "            \"අම්මා {} ගෙනාවා අපි එකට කමු\",\n",
    "            \"හමුවීම {} ට නියමිතයි කාලයට එන්න\",\n",
    "            \"චිත්‍රපටය හොඳයි {} එකට බලමු\",\n",
    "            \"පොත් කියවන්න {} ගැන කියන්න\",\n",
    "            \"කොහොමද {} ගැන පරණ කතා කරමු\"\n",
    "        ]\n",
    "        \n",
    "        # Generate additional examples\n",
    "        items = ['ජංගම දුරකථනයක්', 'මුදල්', 'ත්‍යාගයක්', 'තෑග්ගක්', 'දිනුම්', 'ප්‍රමාණයක්']\n",
    "        \n",
    "        for template in spam_templates:\n",
    "            for item in items[:3]:  # Use first 3 items\n",
    "                Messages.append({\n",
    "                    'Message': template.format(item),\n",
    "                    'Label': 1\n",
    "                })\n",
    "        \n",
    "        for template in ham_templates:\n",
    "            for item in items[3:]:  # Use last 3 items\n",
    "                Messages.append({\n",
    "                    'Message': template.format(item),\n",
    "                    'Label': 0\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        enhanced_df = pd.DataFrame(Messages)\n",
    "        \n",
    "        # Combine with original data if it exists and has both classes\n",
    "        if len(original_df) > 0 and len(original_df['Label'].unique()) > 1:\n",
    "            enhanced_df = pd.concat([original_df, enhanced_df], ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates and ensure balanced classes\n",
    "        enhanced_df = enhanced_df.drop_duplicates(subset=['Message'])\n",
    "        \n",
    "        # Balance the dataset\n",
    "        spam_count = len(enhanced_df[enhanced_df['Label'] == 1])\n",
    "        ham_count = len(enhanced_df[enhanced_df['Label'] == 0])\n",
    "        \n",
    "        if spam_count < ham_count:\n",
    "            # Add more spam examples\n",
    "            spam_df = enhanced_df[enhanced_df['Label'] == 1]\n",
    "            additional_spam = spam_df.sample(n=min(ham_count - spam_count, len(spam_df)), replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_spam], ignore_index=True)\n",
    "        elif ham_count < spam_count:\n",
    "            # Add more ham examples\n",
    "            ham_df = enhanced_df[enhanced_df['Label'] == 0]\n",
    "            additional_ham = ham_df.sample(n=min(spam_count - ham_count, len(ham_df)), replace=True)\n",
    "            enhanced_df = pd.concat([enhanced_df, additional_ham], ignore_index=True)\n",
    "        \n",
    "        return enhanced_df\n",
    "    \n",
    "    def train_english_model(self, df):\n",
    "        \"\"\"Train improved English spam detection model\"\"\"\n",
    "        print(\"Training English model with enhanced features...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.copy()\n",
    "        df['Message'] = df['Message'].fillna('')\n",
    "        df = df[df['Message'].str.strip() != '']\n",
    "        \n",
    "        # Preprocess text\n",
    "        df['processed_Message'] = df['Message'].apply(lambda x: self.preprocess_text(x, 'en'))\n",
    "        \n",
    "        # Extract additional features\n",
    "        additional_features = np.array([self.extract_features(text, 'en') for text in df['Message']])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['processed_Message'], df['Label'], test_size=0.2, random_state=42, stratify=df['Label']\n",
    "        )\n",
    "        \n",
    "        X_train_features, X_test_features = train_test_split(\n",
    "            additional_features, test_size=0.2, random_state=42, stratify=df['Label']\n",
    "        )\n",
    "        \n",
    "        # Create and train TF-IDF vectorizer with optimized parameters\n",
    "        self.english_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        X_train_tfidf = self.english_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = self.english_vectorizer.transform(X_test)\n",
    "        \n",
    "        # Scale additional features using MinMaxScaler to ensure non-negative values\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        X_train_features_scaled = feature_scaler.fit_transform(X_train_features)\n",
    "        X_test_features_scaled = feature_scaler.transform(X_test_features)\n",
    "        self.feature_scaler = feature_scaler\n",
    "        \n",
    "        # Combine TF-IDF features with additional features\n",
    "        from scipy.sparse import hstack\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_features_scaled])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_features_scaled])\n",
    "        \n",
    "        # Create ensemble model - Use only algorithms that work with sparse matrices\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        lr_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "        # For MultinomialNB, use only TF-IDF features (non-negative)\n",
    "        nb_model = MultinomialNB(alpha=0.1)\n",
    "        nb_model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Create voting classifier with compatible models\n",
    "        self.english_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('lr', lr_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Train the ensemble model\n",
    "        self.english_model.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Store the separate NB model for prediction combination\n",
    "        self.english_nb_model = nb_model\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred_ensemble = self.english_model.predict(X_test_combined)\n",
    "        y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "        \n",
    "        # Combine predictions (simple averaging)\n",
    "        y_pred_ensemble_proba = self.english_model.predict_proba(X_test_combined)\n",
    "        y_pred_nb_proba = nb_model.predict_proba(X_test_tfidf)\n",
    "        y_pred_combined_proba = (y_pred_ensemble_proba + y_pred_nb_proba) / 2\n",
    "        y_pred_combined = (y_pred_combined_proba[:, 1] > 0.5).astype(int)\n",
    "        \n",
    "        print(\"English Model Performance:\")\n",
    "        print(f\"Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "        print(f\"Combined Accuracy: {accuracy_score(y_test, y_pred_combined):.4f}\")\n",
    "        print(\"\\nClassification Report (Combined):\")\n",
    "        print(classification_report(y_test, y_pred_combined))\n",
    "        \n",
    "        return self.english_model\n",
    "\n",
    "    def train_sinhala_model(self, df):\n",
    "        \"\"\"Train improved Sinhala spam detection model\"\"\"\n",
    "        print(\"Training Sinhala model with enhanced features...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.copy()\n",
    "        df['Message'] = df['Message'].fillna('')\n",
    "        df = df[df['Message'].str.strip() != '']\n",
    "        \n",
    "        # Create enhanced dataset\n",
    "        df = self.create_realistic_sinhala_dataset(df)\n",
    "        \n",
    "        print(f\"Enhanced Sinhala dataset: {len(df)} samples\")\n",
    "        print(f\"Label distribution: {df['Label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Extract additional features\n",
    "        additional_features = np.array([self.extract_features(text, 'si') for text in df['Message']])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['Message'], df['Label'], test_size=0.2, random_state=42, stratify=df['Label']\n",
    "        )\n",
    "        \n",
    "        X_train_features, X_test_features = train_test_split(\n",
    "            additional_features, test_size=0.2, random_state=42, stratify=df['Label']\n",
    "        )\n",
    "        \n",
    "        # Create TF-IDF vectorizer optimized for Sinhala\n",
    "        self.sinhala_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(1, 4),\n",
    "            max_features=3000,\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        X_train_tfidf = self.sinhala_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = self.sinhala_vectorizer.transform(X_test)\n",
    "        \n",
    "        # Scale additional features using MinMaxScaler\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        X_train_features_scaled = feature_scaler.fit_transform(X_train_features)\n",
    "        X_test_features_scaled = feature_scaler.transform(X_test_features)\n",
    "        \n",
    "        # Combine features\n",
    "        from scipy.sparse import hstack\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_features_scaled])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_features_scaled])\n",
    "        \n",
    "        # Use ensemble model\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        lr_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "        # Separate MultinomialNB for TF-IDF only\n",
    "        nb_model = MultinomialNB(alpha=0.1)\n",
    "        nb_model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        self.sinhala_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('lr', lr_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.sinhala_model.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Store the separate NB model\n",
    "        self.sinhala_nb_model = nb_model\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred_ensemble = self.sinhala_model.predict(X_test_combined)\n",
    "        y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "        \n",
    "        # Combine predictions\n",
    "        y_pred_ensemble_proba = self.sinhala_model.predict_proba(X_test_combined)\n",
    "        y_pred_nb_proba = nb_model.predict_proba(X_test_tfidf)\n",
    "        y_pred_combined_proba = (y_pred_ensemble_proba + y_pred_nb_proba) / 2\n",
    "        y_pred_combined = (y_pred_combined_proba[:, 1] > 0.5).astype(int)\n",
    "        \n",
    "        print(\"Sinhala Model Performance:\")\n",
    "        print(f\"Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "        print(f\"Combined Accuracy: {accuracy_score(y_test, y_pred_combined):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred_combined))\n",
    "        \n",
    "        return self.sinhala_model\n",
    "\n",
    "    def predict(self, text, language='en'):\n",
    "        \"\"\"Make prediction with confidence score\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        if language == 'en':\n",
    "            processed_text = self.preprocess_text(text, 'en')\n",
    "            text_features = self.english_vectorizer.transform([processed_text])\n",
    "            additional_features = np.array([self.extract_features(text, 'en')])\n",
    "            additional_features_scaled = self.feature_scaler.transform(additional_features)\n",
    "            \n",
    "            from scipy.sparse import hstack\n",
    "            combined_features = hstack([text_features, additional_features_scaled])\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            ensemble_pred_proba = self.english_model.predict_proba(combined_features)[0]\n",
    "            nb_pred_proba = self.english_nb_model.predict_proba(text_features)[0]\n",
    "            \n",
    "            # Combine predictions\n",
    "            combined_proba = (ensemble_pred_proba + nb_pred_proba) / 2\n",
    "            prediction = (combined_proba[1] > 0.5).astype(int)\n",
    "            confidence = max(combined_proba)\n",
    "            \n",
    "        else:  # Sinhala\n",
    "            text_features = self.sinhala_vectorizer.transform([text])\n",
    "            additional_features = np.array([self.extract_features(text, 'si')])\n",
    "            additional_features_scaled = self.feature_scaler.transform(additional_features)\n",
    "            \n",
    "            from scipy.sparse import hstack\n",
    "            combined_features = hstack([text_features, additional_features_scaled])\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            ensemble_pred_proba = self.sinhala_model.predict_proba(combined_features)[0]\n",
    "            nb_pred_proba = self.sinhala_nb_model.predict_proba(text_features)[0]\n",
    "            \n",
    "            # Combine predictions\n",
    "            combined_proba = (ensemble_pred_proba + nb_pred_proba) / 2\n",
    "            prediction = (combined_proba[1] > 0.5).astype(int)\n",
    "            confidence = max(combined_proba)\n",
    "        \n",
    "        return prediction, confidence\n",
    "\n",
    "    def save_models(self):\n",
    "        \"\"\"Save all models and vectorizers\"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.english_model, 'improved_english_spam_model.pkl')\n",
    "            joblib.dump(self.sinhala_model, 'improved_sinhala_spam_model.pkl')\n",
    "            joblib.dump(self.english_vectorizer, 'improved_english_vectorizer.pkl')\n",
    "            joblib.dump(self.sinhala_vectorizer, 'improved_sinhala_vectorizer.pkl')\n",
    "            joblib.dump(self.feature_scaler, 'feature_scaler.pkl')\n",
    "            \n",
    "            # Save the separate NB models\n",
    "            if hasattr(self, 'english_nb_model'):\n",
    "                joblib.dump(self.english_nb_model, 'english_nb_model.pkl')\n",
    "            if hasattr(self, 'sinhala_nb_model'):\n",
    "                joblib.dump(self.sinhala_nb_model, 'sinhala_nb_model.pkl')\n",
    "            \n",
    "            print(\"All models saved successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving models: {e}\")\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        try:\n",
    "            self.english_model = joblib.load('improved_english_spam_model.pkl')\n",
    "            self.sinhala_model = joblib.load('improved_sinhala_spam_model.pkl')\n",
    "            self.english_vectorizer = joblib.load('improved_english_vectorizer.pkl')\n",
    "            self.sinhala_vectorizer = joblib.load('improved_sinhala_vectorizer.pkl')\n",
    "            self.feature_scaler = joblib.load('feature_scaler.pkl')\n",
    "            \n",
    "            # Load the separate NB models\n",
    "            try:\n",
    "                self.english_nb_model = joblib.load('english_nb_model.pkl')\n",
    "                self.sinhala_nb_model = joblib.load('sinhala_nb_model.pkl')\n",
    "            except:\n",
    "                print(\"Warning: Separate NB models not found. Using ensemble only.\")\n",
    "                \n",
    "            print(\"All models loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "\n",
    "# Training script\n",
    "def main():\n",
    "    # Initialize detector\n",
    "    detector = ImprovedSpamDetector()\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare English dataset\n",
    "        print(\"Loading English dataset...\")\n",
    "        eng_df = pd.read_csv(\"English_spam.csv\", encoding=\"latin-1\")\n",
    "        eng_df = eng_df[['v1', 'v2']]\n",
    "        eng_df.columns = ['Label', 'Message']\n",
    "        eng_df['Label'] = eng_df['Label'].map({'ham': 0, 'spam': 1})\n",
    "        \n",
    "        # Remove any rows with missing Labels\n",
    "        eng_df = eng_df.dropna(subset=['Label'])\n",
    "        \n",
    "        print(f\"English dataset loaded: {len(eng_df)} samples\")\n",
    "        print(f\"Label distribution: {eng_df['Label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Train English model\n",
    "        detector.train_english_model(eng_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with English dataset: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare Sinhala dataset\n",
    "        print(\"\\nLoading Sinhala dataset...\")\n",
    "        sin_df = pd.read_excel(\"sinhala spam.xlsx\")\n",
    "        sin_df.columns = ['Label', 'Message']\n",
    "        sin_df.dropna(subset=['Label', 'Message'], inplace=True)\n",
    "        sin_df['Label'] = sin_df['Label'].map({'ham': 0, 'spam': 1})\n",
    "        sin_df.dropna(subset=['Label'], inplace=True)\n",
    "        sin_df['Label'] = sin_df['Label'].astype(int)\n",
    "        \n",
    "        print(f\"Sinhala dataset loaded: {len(sin_df)} samples\")\n",
    "        print(f\"Label distribution: {sin_df['Label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Train Sinhala model\n",
    "        detector.train_sinhala_model(sin_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Sinhala dataset: {e}\")\n",
    "    \n",
    "    # Save models\n",
    "    detector.save_models()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed! Models saved.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Example usage function\n",
    "def test_detector():\n",
    "    \"\"\"Test the detector with sample Messages\"\"\"\n",
    "    detector = ImprovedSpamDetector()\n",
    "    \n",
    "    try:\n",
    "        detector.load_models()\n",
    "        \n",
    "        # Test Messages\n",
    "        test_Messages = [\n",
    "            (\"Congratulations! You've won $1000! Click here now!\", 'en'),\n",
    "            (\"Hi, how are you doing today?\", 'en'),\n",
    "            (\"FREE OFFER! Limited time deal!\", 'en'),\n",
    "            (\"Meeting at 3pm tomorrow\", 'en')\n",
    "        ]\n",
    "        \n",
    "        print(\"Testing detector:\")\n",
    "        for Message, lang in test_Messages:\n",
    "            prediction, confidence = detector.predict(Message, lang)\n",
    "            result = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "            print(f\"Message: '{Message}'\")\n",
    "            print(f\"Prediction: {result} (Confidence: {confidence:.4f})\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing detector: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment the line below to test the detector after training\n",
    "    # test_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
