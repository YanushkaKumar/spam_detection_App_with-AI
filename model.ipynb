{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1331586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading English dataset...\n",
      "English dataset loaded: 5572 samples\n",
      "Label distribution: {0: 4825, 1: 747}\n",
      "Training English model with enhanced features...\n",
      "English Model Performance:\n",
      "Ensemble Accuracy: 0.9874\n",
      "Combined Accuracy: 0.9892\n",
      "\n",
      "Classification Report (Combined):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       966\n",
      "           1       1.00      0.92      0.96       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.96      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n",
      "\n",
      "Loading Sinhala dataset...\n",
      "Sinhala dataset loaded: 87 samples\n",
      "Label distribution: {0: 87}\n",
      "Training Sinhala model with enhanced features...\n",
      "Warning: Only one class found in Sinhala dataset. Attempting to augment with synthetic data.\n",
      "Sinhala Model Performance:\n",
      "Ensemble Accuracy: 1.0000\n",
      "NB Accuracy: 0.5429\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        18\n",
      "           1       1.00      1.00      1.00        17\n",
      "\n",
      "    accuracy                           1.00        35\n",
      "   macro avg       1.00      1.00      1.00        35\n",
      "weighted avg       1.00      1.00      1.00        35\n",
      "\n",
      "All models saved successfully!\n",
      "\n",
      "==================================================\n",
      "Training completed! Models saved.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Improved Spam Detection Model with Enhanced Features - FIXED\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "# Try to import NLTK components with fallback\n",
    "try:\n",
    "    import nltk\n",
    "    # Try to download NLTK data\n",
    "    try:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        NLTK_AVAILABLE = True\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: NLTK download failed: {e}\")\n",
    "        print(\"Falling back to basic text processing...\")\n",
    "        NLTK_AVAILABLE = False\n",
    "except ImportError:\n",
    "    print(\"Warning: NLTK not available. Using basic text processing...\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "class ImprovedSpamDetector:\n",
    "    def __init__(self):\n",
    "        self.english_model = None\n",
    "        self.sinhala_model = None\n",
    "        self.english_vectorizer = None\n",
    "        self.sinhala_vectorizer = None\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        \n",
    "    def extract_features(self, text, language='en'):\n",
    "        \"\"\"Extract additional features from text\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Basic text statistics\n",
    "        features['length'] = len(text)\n",
    "        features['word_count'] = len(text.split())\n",
    "        features['char_count'] = len([c for c in text if c.isalpha()])\n",
    "        features['digit_count'] = len([c for c in text if c.isdigit()])\n",
    "        features['upper_case_count'] = len([c for c in text if c.isupper()])\n",
    "        features['punctuation_count'] = len([c for c in text if c in string.punctuation])\n",
    "        \n",
    "        # Ratios (avoid division by zero)\n",
    "        if len(text) > 0:\n",
    "            features['digit_ratio'] = features['digit_count'] / len(text)\n",
    "            features['upper_ratio'] = features['upper_case_count'] / len(text)\n",
    "            features['punct_ratio'] = features['punctuation_count'] / len(text)\n",
    "        else:\n",
    "            features['digit_ratio'] = 0\n",
    "            features['upper_ratio'] = 0\n",
    "            features['punct_ratio'] = 0\n",
    "            \n",
    "        # Special characters\n",
    "        features['exclamation_count'] = text.count('!')\n",
    "        features['question_count'] = text.count('?')\n",
    "        features['dollar_count'] = text.count('$')\n",
    "        features['url_count'] = len(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text))\n",
    "        features['email_count'] = len(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text))\n",
    "        features['phone_count'] = len(re.findall(r'\\b\\d{3}-\\d{3}-\\d{4}\\b|\\b\\d{10}\\b', text))\n",
    "        \n",
    "        # Spam keywords (you can expand this list based on your domain)\n",
    "        spam_keywords_en = ['free', 'win', 'winner', 'cash', 'prize', 'urgent', 'congratulations', \n",
    "                           'offer', 'deal', 'discount', 'limited', 'act now', 'call now', 'click here']\n",
    "        spam_keywords_si = ['නොමිලේ', 'ජයග්‍රහණය', 'ත්‍යාගය', 'මුදල්', 'හදිසි', 'සුභ පැතුම්']\n",
    "        \n",
    "        if language == 'en':\n",
    "            spam_words = spam_keywords_en\n",
    "        else:\n",
    "            spam_words = spam_keywords_si\n",
    "            \n",
    "        text_lower = text.lower()\n",
    "        features['spam_keywords_count'] = sum(1 for word in spam_words if word in text_lower)\n",
    "        \n",
    "        # Average word length\n",
    "        words = text.split()\n",
    "        if words:\n",
    "            features['avg_word_length'] = sum(len(word) for word in words) / len(words)\n",
    "        else:\n",
    "            features['avg_word_length'] = 0\n",
    "            \n",
    "        return list(features.values())\n",
    "    \n",
    "    def basic_tokenize(self, text):\n",
    "        \"\"\"Basic tokenization fallback when NLTK is not available\"\"\"\n",
    "        # Simple word tokenization using regex\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        return words\n",
    "    \n",
    "    def get_english_stopwords(self):\n",
    "        \"\"\"Get English stopwords with fallback\"\"\"\n",
    "        if NLTK_AVAILABLE:\n",
    "            try:\n",
    "                return set(stopwords.words('english'))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback stopwords list\n",
    "        return {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', \n",
    "            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', \n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', \n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "            'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after', \n",
    "            'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "            'further', 'then', 'once'\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text, language='en'):\n",
    "        \"\"\"Enhanced text preprocessing with NLTK fallback\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # For English, remove stopwords but keep some important ones for spam detection\n",
    "        if language == 'en':\n",
    "            stop_words = self.get_english_stopwords()\n",
    "            # Keep these words as they might be important for spam detection\n",
    "            keep_words = {'free', 'win', 'money', 'cash', 'offer', 'deal', 'urgent', 'now'}\n",
    "            stop_words = stop_words - keep_words\n",
    "            \n",
    "            if NLTK_AVAILABLE:\n",
    "                try:\n",
    "                    words = word_tokenize(text)\n",
    "                except:\n",
    "                    words = self.basic_tokenize(text)\n",
    "            else:\n",
    "                words = self.basic_tokenize(text)\n",
    "                \n",
    "            text = ' '.join([word for word in words if word not in stop_words])\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def train_english_model(self, df):\n",
    "        \"\"\"Train improved English spam detection model\"\"\"\n",
    "        print(\"Training English model with enhanced features...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.copy()\n",
    "        df['message'] = df['message'].fillna('')\n",
    "        df = df[df['message'].str.strip() != '']\n",
    "        \n",
    "        # Preprocess text\n",
    "        df['processed_message'] = df['message'].apply(lambda x: self.preprocess_text(x, 'en'))\n",
    "        \n",
    "        # Extract additional features\n",
    "        additional_features = np.array([self.extract_features(text, 'en') for text in df['message']])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['processed_message'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    "        )\n",
    "        \n",
    "        X_train_features, X_test_features = train_test_split(\n",
    "            additional_features, test_size=0.2, random_state=42, stratify=df['label']\n",
    "        )\n",
    "        \n",
    "        # Create and train TF-IDF vectorizer with optimized parameters\n",
    "        self.english_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        X_train_tfidf = self.english_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = self.english_vectorizer.transform(X_test)\n",
    "        \n",
    "        # Scale additional features using MinMaxScaler to ensure non-negative values\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        X_train_features_scaled = feature_scaler.fit_transform(X_train_features)\n",
    "        X_test_features_scaled = feature_scaler.transform(X_test_features)\n",
    "        self.feature_scaler = feature_scaler\n",
    "        \n",
    "        # Combine TF-IDF features with additional features\n",
    "        from scipy.sparse import hstack\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_features_scaled])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_features_scaled])\n",
    "        \n",
    "        # Create ensemble model - Use only algorithms that work with sparse matrices\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        lr_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "        # For MultinomialNB, use only TF-IDF features (non-negative)\n",
    "        nb_model = MultinomialNB(alpha=0.1)\n",
    "        nb_model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        # Create voting classifier with compatible models\n",
    "        self.english_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('lr', lr_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Train the ensemble model\n",
    "        self.english_model.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Store the separate NB model for prediction combination\n",
    "        self.english_nb_model = nb_model\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred_ensemble = self.english_model.predict(X_test_combined)\n",
    "        y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "        \n",
    "        # Combine predictions (simple averaging)\n",
    "        y_pred_ensemble_proba = self.english_model.predict_proba(X_test_combined)\n",
    "        y_pred_nb_proba = nb_model.predict_proba(X_test_tfidf)\n",
    "        y_pred_combined_proba = (y_pred_ensemble_proba + y_pred_nb_proba) / 2\n",
    "        y_pred_combined = (y_pred_combined_proba[:, 1] > 0.5).astype(int)\n",
    "        \n",
    "        print(\"English Model Performance:\")\n",
    "        print(f\"Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "        print(f\"Combined Accuracy: {accuracy_score(y_test, y_pred_combined):.4f}\")\n",
    "        print(\"\\nClassification Report (Combined):\")\n",
    "        print(classification_report(y_test, y_pred_combined))\n",
    "        \n",
    "        return self.english_model\n",
    "\n",
    "    def train_sinhala_model(self, df):\n",
    "        \"\"\"Train improved Sinhala spam detection model\"\"\"\n",
    "        print(\"Training Sinhala model with enhanced features...\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.copy()\n",
    "        df['message'] = df['message'].fillna('')\n",
    "        df = df[df['message'].str.strip() != '']\n",
    "        \n",
    "        # Check if we have both classes, if not, augment with synthetic spam data\n",
    "        if len(df['label'].unique()) < 2:\n",
    "            print(\"Warning: Only one class found in Sinhala dataset. Attempting to augment with synthetic data.\")\n",
    "            from sklearn.utils import resample\n",
    "            # Create synthetic spam samples (label 1) based on existing ham (label 0) with added spam keywords\n",
    "            spam_keywords_si = ['නොමිලේ', 'ජයග්‍රහණය', 'ත්‍යාගය', 'මුදල්', 'හදිසි', 'සුභ පැතුම්']\n",
    "            synthetic_spam = df[df['label'] == 0].copy()\n",
    "            synthetic_spam['message'] = synthetic_spam['message'].apply(\n",
    "                lambda x: x + ' ' + ' '.join(np.random.choice(spam_keywords_si, size=2, replace=False))\n",
    "            )\n",
    "            synthetic_spam['label'] = 1\n",
    "            df = pd.concat([df, synthetic_spam]).reset_index(drop=True)\n",
    "\n",
    "        # Extract additional features\n",
    "        additional_features = np.array([self.extract_features(text, 'si') for text in df['message']])\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            df['message'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
    "        )\n",
    "        \n",
    "        X_train_features, X_test_features = train_test_split(\n",
    "            additional_features, test_size=0.2, random_state=42, stratify=df['label']\n",
    "        )\n",
    "        \n",
    "        # Create TF-IDF vectorizer optimized for Sinhala\n",
    "        self.sinhala_vectorizer = TfidfVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=(1, 4),\n",
    "            max_features=3000,\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True\n",
    "        )\n",
    "        \n",
    "        X_train_tfidf = self.sinhala_vectorizer.fit_transform(X_train)\n",
    "        X_test_tfidf = self.sinhala_vectorizer.transform(X_test)\n",
    "        \n",
    "        # Scale additional features using MinMaxScaler\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        feature_scaler = MinMaxScaler()\n",
    "        X_train_features_scaled = feature_scaler.fit_transform(X_train_features)\n",
    "        X_test_features_scaled = feature_scaler.transform(X_test_features)\n",
    "        \n",
    "        # Combine features\n",
    "        from scipy.sparse import hstack\n",
    "        X_train_combined = hstack([X_train_tfidf, X_train_features_scaled])\n",
    "        X_test_combined = hstack([X_test_tfidf, X_test_features_scaled])\n",
    "        \n",
    "        # Use ensemble model\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        lr_model = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "        # Separate MultinomialNB for TF-IDF only\n",
    "        nb_model = MultinomialNB(alpha=0.1)\n",
    "        nb_model.fit(X_train_tfidf, y_train)\n",
    "        \n",
    "        self.sinhala_model = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', rf_model),\n",
    "                ('lr', lr_model)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        self.sinhala_model.fit(X_train_combined, y_train)\n",
    "        \n",
    "        # Store the separate NB model\n",
    "        self.sinhala_nb_model = nb_model\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred_ensemble = self.sinhala_model.predict(X_test_combined)\n",
    "        y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "        \n",
    "        print(\"Sinhala Model Performance:\")\n",
    "        print(f\"Ensemble Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}\")\n",
    "        print(f\"NB Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred_ensemble))\n",
    "        \n",
    "        return self.sinhala_model\n",
    "\n",
    "    def predict(self, text, language='en'):\n",
    "        \"\"\"Make prediction with confidence score\"\"\"\n",
    "        # Convert to string if not already\n",
    "        text = str(text) if text is not None else \"\"\n",
    "        \n",
    "        if language == 'en':\n",
    "            processed_text = self.preprocess_text(text, 'en')\n",
    "            text_features = self.english_vectorizer.transform([processed_text])\n",
    "            additional_features = np.array([self.extract_features(text, 'en')])\n",
    "            additional_features_scaled = self.feature_scaler.transform(additional_features)\n",
    "            \n",
    "            from scipy.sparse import hstack\n",
    "            combined_features = hstack([text_features, additional_features_scaled])\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            ensemble_pred_proba = self.english_model.predict_proba(combined_features)[0]\n",
    "            nb_pred_proba = self.english_nb_model.predict_proba(text_features)[0]\n",
    "            \n",
    "            # Combine predictions\n",
    "            combined_proba = (ensemble_pred_proba + nb_pred_proba) / 2\n",
    "            prediction = (combined_proba[1] > 0.5).astype(int)\n",
    "            confidence = max(combined_proba)\n",
    "            \n",
    "        else:  # Sinhala\n",
    "            text_features = self.sinhala_vectorizer.transform([text])\n",
    "            \n",
    "            # Check if it's a dummy classifier\n",
    "            if hasattr(self.sinhala_model, 'strategy'):\n",
    "                prediction = self.sinhala_model.predict(text.reshape(-1, 1))[0]\n",
    "                confidence = 0.5  # Default confidence for dummy classifier\n",
    "            else:\n",
    "                additional_features = np.array([self.extract_features(text, 'si')])\n",
    "                additional_features_scaled = self.feature_scaler.transform(additional_features)\n",
    "                \n",
    "                from scipy.sparse import hstack\n",
    "                combined_features = hstack([text_features, additional_features_scaled])\n",
    "                \n",
    "                # Get predictions from both models\n",
    "                ensemble_pred_proba = self.sinhala_model.predict_proba(combined_features)[0]\n",
    "                nb_pred_proba = self.sinhala_nb_model.predict_proba(text_features)[0]\n",
    "                \n",
    "                # Combine predictions\n",
    "                combined_proba = (ensemble_pred_proba + nb_pred_proba) / 2\n",
    "                prediction = (combined_proba[1] > 0.5).astype(int)\n",
    "                confidence = max(combined_proba)\n",
    "        \n",
    "        return prediction, confidence\n",
    "\n",
    "    def save_models(self):\n",
    "        \"\"\"Save all models and vectorizers\"\"\"\n",
    "        try:\n",
    "            joblib.dump(self.english_model, 'improved_english_spam_model.pkl')\n",
    "            joblib.dump(self.sinhala_model, 'improved_sinhala_spam_model.pkl')\n",
    "            joblib.dump(self.english_vectorizer, 'improved_english_vectorizer.pkl')\n",
    "            joblib.dump(self.sinhala_vectorizer, 'improved_sinhala_vectorizer.pkl')\n",
    "            joblib.dump(self.feature_scaler, 'feature_scaler.pkl')\n",
    "            \n",
    "            # Save the separate NB models\n",
    "            if hasattr(self, 'english_nb_model'):\n",
    "                joblib.dump(self.english_nb_model, 'english_nb_model.pkl')\n",
    "            if hasattr(self, 'sinhala_nb_model'):\n",
    "                joblib.dump(self.sinhala_nb_model, 'sinhala_nb_model.pkl')\n",
    "            \n",
    "            print(\"All models saved successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving models: {e}\")\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Load pre-trained models\"\"\"\n",
    "        try:\n",
    "            self.english_model = joblib.load('improved_english_spam_model.pkl')\n",
    "            self.sinhala_model = joblib.load('improved_sinhala_spam_model.pkl')\n",
    "            self.english_vectorizer = joblib.load('improved_english_vectorizer.pkl')\n",
    "            self.sinhala_vectorizer = joblib.load('improved_sinhala_vectorizer.pkl')\n",
    "            self.feature_scaler = joblib.load('feature_scaler.pkl')\n",
    "            \n",
    "            # Load the separate NB models\n",
    "            try:\n",
    "                self.english_nb_model = joblib.load('english_nb_model.pkl')\n",
    "                self.sinhala_nb_model = joblib.load('sinhala_nb_model.pkl')\n",
    "            except:\n",
    "                print(\"Warning: Separate NB models not found. Using ensemble only.\")\n",
    "                \n",
    "            print(\"All models loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "\n",
    "# Training script\n",
    "def main():\n",
    "    # Initialize detector\n",
    "    detector = ImprovedSpamDetector()\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare English dataset\n",
    "        print(\"Loading English dataset...\")\n",
    "        eng_df = pd.read_csv(\"English_spam.csv\", encoding=\"latin-1\")\n",
    "        eng_df = eng_df[['v1', 'v2']]\n",
    "        eng_df.columns = ['label', 'message']\n",
    "        eng_df['label'] = eng_df['label'].map({'ham': 0, 'spam': 1})\n",
    "        \n",
    "        # Remove any rows with missing labels\n",
    "        eng_df = eng_df.dropna(subset=['label'])\n",
    "        \n",
    "        print(f\"English dataset loaded: {len(eng_df)} samples\")\n",
    "        print(f\"Label distribution: {eng_df['label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Train English model\n",
    "        detector.train_english_model(eng_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with English dataset: {e}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare Sinhala dataset\n",
    "        print(\"\\nLoading Sinhala dataset...\")\n",
    "        sin_df = pd.read_excel(\"sinhala spam.xlsx\")\n",
    "        sin_df.columns = ['label', 'message']\n",
    "        sin_df.dropna(subset=['label', 'message'], inplace=True)\n",
    "        sin_df['label'] = sin_df['label'].map({'Ham': 0, 'Spam': 1})\n",
    "        sin_df.dropna(subset=['label'], inplace=True)\n",
    "        sin_df['label'] = sin_df['label'].astype(int)\n",
    "        \n",
    "        print(f\"Sinhala dataset loaded: {len(sin_df)} samples\")\n",
    "        print(f\"Label distribution: {sin_df['label'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Train Sinhala model\n",
    "        detector.train_sinhala_model(sin_df)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with Sinhala dataset: {e}\")\n",
    "    \n",
    "    # Save models\n",
    "    detector.save_models()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed! Models saved.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Example usage function\n",
    "def test_detector():\n",
    "    \"\"\"Test the detector with sample messages\"\"\"\n",
    "    detector = ImprovedSpamDetector()\n",
    "    \n",
    "    try:\n",
    "        detector.load_models()\n",
    "        \n",
    "        # Test messages\n",
    "        test_messages = [\n",
    "            (\"Congratulations! You've won $1000! Click here now!\", 'en'),\n",
    "            (\"Hi, how are you doing today?\", 'en'),\n",
    "            (\"FREE OFFER! Limited time deal!\", 'en'),\n",
    "            (\"Meeting at 3pm tomorrow\", 'en')\n",
    "        ]\n",
    "        \n",
    "        print(\"Testing detector:\")\n",
    "        for message, lang in test_messages:\n",
    "            prediction, confidence = detector.predict(message, lang)\n",
    "            result = \"SPAM\" if prediction == 1 else \"HAM\"\n",
    "            print(f\"Message: '{message}'\")\n",
    "            print(f\"Prediction: {result} (Confidence: {confidence:.4f})\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing detector: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # Uncomment the line below to test the detector after training\n",
    "    # test_detector()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
